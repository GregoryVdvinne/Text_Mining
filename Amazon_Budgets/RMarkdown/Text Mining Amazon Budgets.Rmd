---
title: "Text Mining Amazon Annual Reports"
author: "Gregory Vander Vinne"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: false
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
# Setup ------------------------------------------------------------------------------------

#Clear memory
rm(list = ls(all=T))


# Load libraries and install if not installed already
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,  # Grammar for data and graphics
  here,       # Relative file paths
  tidytext,   # Text analysis in a tidy format
  feather,    # Files that are fast to write and read
  ggrepel,    # Repulsive text labels in ggplot2
  ggtext,     # Fancy text in ggplot2
  knitr,      # For kable
  gridExtra   # Arrange multiple plots into one
)


#Set some output options
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 7)


# Read tidy data from all years to start from this point in the script
all_budgets <- read_feather(here("Amazon_Budgets/Data/Intermediate/all_budgets_tidy_uncleaned.feather"))



# Amazon Color Palette
my_pal <- c("#ff9900", "#146eb4", "#232f3e", "#f2f2f2", "#000000")

line_colour = my_pal[3] 
weak_text = my_pal[3]
strong_text = my_pal[5]
back_colour = my_pal[4]


# Define my ggplot theme as a funciton to avoid repetitive code
my_theme <- function(base_size = 10) {
  theme_minimal(base_size = base_size) +
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = back_colour,
                                    color = back_colour),
    plot.background = element_rect(fill = back_colour, 
                                   colour = back_colour),
    plot.caption.position = "plot",
    plot.title.position = "plot",
    plot.title = element_textbox_simple(size = rel(2),
                                        # family = main_font,
                                        color = strong_text,
                                        margin = margin(4, 0, 10, 4)),
    plot.subtitle = element_textbox_simple(size = rel(1.25),
                                           # family = main_font,
                                           colour = weak_text,
                                           margin = margin(0, 4, 6, 4)), 
    axis.title.y = element_text(size = rel(1.2),
                                # family = main_font,
                                colour = strong_text, 
                                margin = margin(0, 6 , 0, 4)),
    axis.title.x = element_text(size = rel(1.2),
                                # family = main_font,
                                colour = strong_text, 
                                margin = margin(6, 0 , 2, 0)),
    axis.text = element_text(size = rel(1.1),
                             # family = main_font,
                             colour = weak_text, 
                             margin = margin(0, 0, 0, 6)),
    plot.caption = element_textbox_simple(size = rel(0.8),
                                          colour = weak_text,
                                          # family = main_font,
                                          hjust = 0.5, # Seems to be ignored
                                          margin = margin(4,0,2,4)),
    legend.title = element_text(size = rel(1),
                                # family = main_font,
                                colour = strong_text), 
    legend.text = element_text(size = rel(0.9),
                               # family = main_font,
                               colour = weak_text)
  )

}


```

## Intro

Blah Blah Blah

```{r The Most Common Words}

all_budgets |>
  group_by(word) |>
  summarise("Word Count" = n()) |>
  slice_max(order_by = `Word Count`, n = 10) |>
  kable()


```

## Stop Words and Raw Word Frequencies

We Can start by removing a list of stop words. Stop words are words that are used commonly and are unlikley to be interesting for our analysis. Examples include words such as "the", "it" and "or".

```{r Remove Stop Words}

# Remove Standad list of common words such as "and", "the', "or"
all_budgets_no_stop <-  all_budgets |>
  anti_join(stop_words, by = "word")

# A New List of the Most-Common Words
all_budgets_no_stop |>
  group_by(word) |>
  summarise("Word Count" = n()) |>
  slice_max(order_by = `Word Count`, n = 10) |>
  kable()


```

It is clear from the above table that there are still words that will not be interesting for our analysis, such as individual numbers and the word "million" which appear because of the financial tables in the document. The word "December" appears because Amazon uses a fiscal year end of December 31st. Although I remove the names of all months because I think they will not be very interesting, looking into the word "December" and finding that it appears frequently because it is the month of Amazon's fiscal year-end has given us our first insight, albeit a trivial one. 

```{r Remove Custom List of Stop Words}

# Custom list of stop words to remove
my_stop_words <- c(   
  str_to_lower(month.name), # List of month names
  letters                  # List of letters
)

# Remove above list of custom stop words and anything contain a number
all_budgets_no_stop <-  all_budgets_no_stop |>
  filter(!(word %in% my_stop_words),    # List of custom stop words
        !str_detect(word, "\\d"))       # Anything containing a number

# A New List of the Most-Common Words
all_budgets_no_stop |>
  group_by(word) |>
  summarise("Word Count" = n()) |>
  slice_max(order_by = `Word Count`, n = 10) |>
  kable()


```

### Raw Term Frequencies in the First 9 Reports vs The Last 10 Reports


```{r Scatter of Raw Word Frequencies}

# Prepare data for the scatter plot
scatter_data <- all_budgets_no_stop |>
  mutate(first_second_half = if_else(year %in% 2005:2013, "first_half", "second_half")) |>
  group_by(first_second_half, word) |>
  summarise(word_count = n()) |>
  pivot_wider(names_from = first_second_half, values_from = word_count) |>
  mutate(second_half = second_half * 9/10, # Normalize second half to account for the fact it has one more year
         difference = abs(first_half - second_half)) 
  
# Scatterplot
ggplot(scatter_data, aes(x = first_half, y = second_half)) + 
  geom_point(color = my_pal[1], alpha = 0.65, size = 2.5) + 
  geom_text_repel(label = if_else(scatter_data$first_half > 1000 | scatter_data$second_half > 1000 | scatter_data$difference > 300, scatter_data$word, ""), 
                  color = weak_text) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = line_colour) +  # 45-degree line
  labs(title = "Comparing Word Frequencies Across Two Time Periods", 
       subtitle = "Note that the frequencies of words used in the 2014 to 2023
       period are multiplied by 9/10 to account for the fact that that this period
       is one year longer than the other.",
       x = "2005 to 2013 Annual Reports", y = "2014 to 2023 Annual Reports") + 
  my_theme()

```
From this, we can glean a few interesting pieces of information. First, we see that words like "Cash", "Tax", and "Costs", are used at relatively similar frequencies in both the first nine annual reports and the last ten. However, words like "Billion", and "AWS", are used much more frequently in the last ten years of annual reports than the first nine. The word "Billion" likely appears more in the second half, and million in the first half because the company has grown a great deal, and now deals in billions of dollars rather than millions. "AWS" refers to Amazon Web Services. From this graphic we might guess that Amazon Web Services has become an increasingly important part of the company over time. On the flip side, we see that the words "Stock" and "Compensation" were used more frequently in the first nine annual reports. We might want to investigate this more to understand why.


### Total Word Counts

One attribute of these reports that might cause an issue for our analysis is total word count. If the annual reports get substantially short over time, we will likely see a downward trend in most words that is not actually meaningful. Let's take a look at the word counts of the annual reports by year, after removing the stop words discussed earlier. We see that although the total number of words is generally fairly similar from year to year, the number of words was much lower in 2019 and 2020. Therefore, although I tried to make the two periods in the previous section comparable by normalizing for the number of years, it might be more appropriate to normalize for the total number of words.

```{r Line Graph Total Word Count}

# Do annual reports vary much in word count?
annual_word_count <- all_budgets_no_stop |>
  mutate(year = as.numeric(year)) |>
  group_by(year) |>
  summarise(word_count = n())

  
# Scatterplot
ggplot(annual_word_count, aes(x = year, y = word_count)) + 
  geom_point(color = my_pal[1], alpha = 0.65, size = 3.5) + 
  geom_line(color = my_pal[1], line_width = 0.75) +
  labs(title = "Total Word Counts Over Time", 
       subtitle = "Stop Words Removed",
       x = "", y = "Total Words Exluding Stop Words") + 
  my_theme()

```



### Tracking the Usage of One Word Over Time

Next, let us illustrate the effect of normalizing our frequencies for the total number of words used in each annual report, while also exploring the usage of the term "AWS", which we identified earlier as a word that was used much more frequently from 2014 to 2023 than from 2005 to 2013. To do so, let's look at both the raw frequency of the term by year, and an frequency adjusted for the total number of words in the annual report. The adjustment used involves dividing the occurrences of the word we want to look at by the total word count for a given year, and then multiplying by the average word count across all of the years. 

```{r Raw Line Graph of One Word}

# Prepare data for the line graph
one_word_line_data <- all_budgets_no_stop |>
  group_by(year, word) |>
  mutate(year = as.numeric(year)) |>
  summarise(word_count = n(), .groups = "drop") |>
  filter(word == "aws") |>
  complete(year = full_seq(year, 1), word = "aws", fill = list(word_count = 0)) |> # Otherwise years with 0 observations are lost
  pivot_wider(names_from = word, values_from = word_count)
  
# Scatterplot
raw_freq_p <- ggplot(one_word_line_data, aes(x = year, y = aws)) + 
  geom_line(color = my_pal[1], size= 0.75) +
  geom_point(color = my_pal[1], alpha = 0.65, size = 3.5) + 
  labs(title = "Raw Frequency of The Term AWS Over Time", 
       x = "", y = "Occurences") + 
  my_theme()





# Average Word Count
avg_word_count <- mean(annual_word_count$word_count)

# Join data for just aws with data for total word count
adjusted_word_line_data <- annual_word_count |>
  mutate(year = as.numeric(year)) |>
  full_join(one_word_line_data, by = "year") |>
  replace(is.na(year), 0) |>
  mutate(aws = aws/word_count*avg_word_count)


# Line Graph
adj_freq_p <- ggplot(adjusted_word_line_data, aes(x = year, y = aws)) + 
  geom_line(color = my_pal[1], size= 0.75) +
  geom_point(color = my_pal[1], alpha = 0.65, size = 3.5) + 
  labs(title = "Adjusted Frequency of The Term AWS Over Time", 
       x = "", y = "Word-Count-Adjusted Occurences") + 
  my_theme()

gridExtra::grid.arrange(raw_freq_p, adj_freq_p, 
                        ncol = 1)


```

Although AWS was established in 1997, we see that the term was hardly used in Amazon's annual reports under around 2010, when it started to be used much more. Then, in the 2015 annual report the term was used almost 70 times. AWS seems to have remained an important topic in the reports, with the exceptions of 2019 and 2020. Adjusting the series for the total word counts in the annual reports does not make a terribly large difference. However, the overall upward trend of the usage of the word "AWS" is slightly more discernible once we adjust for total word count. 

## Relative Word Frequencies

By looking at changes in the usage of certain words over time and adjusting for total word counts, we have begun to look at something a little bit more interesting. However, there are more systematic approaches to finding which words are relatively common in which annual reports than what we have done so far. On approach is to combine term frequency (tf) with inverse document frequence (idf) to generate a tf-idf statistic. In the book Text Mining With R, Julia Silge and David Robinson describe tf-idf as follows

*"The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites."*

By looking at which words have the highest tf-idf statistics in a given annual report, we are able to get an idea of which words typify that annual report. 

### Words that Typify The 2023 Annual Report

First, let's look at the words with the highest tf-idf statistics from the 2023 annual report, to see which words are used most in the 2023 annual report *relative* to other years' reports.

```{r tf-idf 2023 Report}





```

## Sentiment Analysis







